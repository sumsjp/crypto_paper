1. [摘要](#S1)
2. [研究角度](#S2)
3. [研究方法](#S3)
4. [研究結果](#S4)
5. [研究結論](#S5)

# Reinforcement Learning Pair Trading: A Dynamic Scaling Approach

<a name="S1"></a>
## 1. **摘要**

加密貨幣是一種基於密碼學的數字資產，其價格極端波動，全球每日交易量約達 **700 億美元**。由於市場的高波動性，使得加密貨幣交易變得極具挑戰性。本研究探討**強化學習（Reinforcement Learning, RL）**是否能夠提升加密貨幣演算法交易的決策能力，並與傳統交易方法進行比較。  

為解決此問題，我們將**強化學習與統計套利交易技術——配對交易（Pair Trading）**相結合，該技術利用統計相關資產間的價格差異進行交易。我們構建了**RL 交易環境**，並訓練 RL 代理（agent）來決定何時及如何交易加密貨幣對。我們針對強化學習開發了**新的獎勵設計（reward shaping）**及**觀察/動作空間（observation/action spaces）**，以提升交易決策的智能化程度。  

在實驗中，我們利用**BTC-GBP 和 BTC-EUR** 交易對的價格數據（時間間隔為 **1 分鐘，n = 263,520**）進行測試。結果顯示，**傳統的非 RL 配對交易技術年化利潤為 8.33%**，而**基於 RL 的配對交易技術年化利潤範圍為 9.94% 至 31.53%**，具體收益率取決於所選用的 RL 演算法。  

實驗結果表明，在**高波動市場**（如加密貨幣市場）中，RL 方法在交易決策上能顯著優於人工或傳統配對交易技術，並能夠適應市場變化，提高交易績效。


<a name="S2"></a>
## 2. **研究角度**

本研究從以下**三個主要角度**探討**強化學習（Reinforcement Learning, RL）**在配對交易（Pair Trading）中的應用與優勢：

**1. 金融市場交易策略與統計套利**
   - **傳統配對交易**：基於統計套利的概念，尋找兩種高度相關的資產，當價格發生偏離時，通過買入低估資產、賣出高估資產來獲利。
   - **高頻交易（HFT）需求**：隨著市場交易速度的提高，傳統的靜態規則交易方法**無法快速適應市場波動**，因此需要更靈活的決策機制。
   - **研究問題**：本研究探討 RL **能否在統計套利的基礎上優化決策，使交易更靈活且更具適應性**，並超越傳統的配對交易策略。

**2. 人工智慧與強化學習在金融交易中的應用**
   - **RL 在交易中的優勢**：
     - **適應市場變化**：傳統交易策略依賴固定規則，而 RL 能夠學習市場模式，**根據歷史數據與市場波動動態調整交易策略**。
     - **自動決策與學習**：RL 代理可以**根據市場情境自動調整投資決策**，包括**交易時機**和**投資金額**。
   - **新技術貢獻**：
     - **RL 環境設計**：構建了一個適用於**配對交易的 RL 交易環境**。
     - **獎勵函數（Reward Shaping）設計**：針對配對交易設計**新的獎勵機制**，優化交易決策過程。
     - **動態調整機制**：提出**RL1（決策交易時機）**與**RL2（決策交易時機與投資金額）**兩種模型，驗證**是否允許 RL 自主決策投資金額能進一步提升交易績效**。

**3. 交易績效評估與實證研究**
   - **實驗設計**：
     - 測試資產對：**BTC-GBP 和 BTC-EUR**。
     - **不同 RL 演算法比較**（PPO、A2C、SAC、DQN）。
     - **與傳統方法比較**（Gatev et al., 2006 及 Kim and Kim, 2019）。
   - **績效評估**：
     - **交易策略的年化利潤**：比較 RL 交易策略與傳統方法的收益率。
     - **交易頻率與市場適應性**：分析 RL 是否比傳統方法更具靈活性。
     - **風險管理與回撤（Drawdown）**：衡量 RL 方法是否能夠更有效地管理市場風險。

**總結**
本研究從**金融交易策略、AI 在交易中的應用、交易績效評估**三個角度，探討 RL 在配對交易中的潛力，並驗證 RL **是否能在高波動市場（如加密貨幣市場）中提高交易績效，超越傳統方法**。結果顯示，RL 方法能夠適應市場變動，並在適當的交易環境下提供更高的回報率，特別是**允許 RL 動態調整投資數量的 RL2 方法，在所有交易策略中表現最佳**。


<a name="S3"></a>
## 3. **研究方法**

本研究採用了**強化學習（Reinforcement Learning, RL）結合配對交易（Pair Trading）**的方法，透過數據分析、環境建構及演算法測試來驗證 RL 在高頻金融交易中的有效性。研究方法主要包含以下幾個步驟：

**1. 配對交易模型建構**
**(1) 資產配對選擇**
   - 透過 **皮爾遜相關係數（Pearson Correlation）** 與 **恩格爾-格蘭傑協整檢定（Engle-Granger Cointegration Test）**，挑選出統計上具有長期相關性的加密貨幣資產組合。
   - 本研究選擇 **BTC-GBP 與 BTC-EUR** 作為主要交易對，並使用 **1 分鐘交易間隔**（共 **263,520 筆數據**）。

**(2) 價差計算（Spread Calculation）**
   - 根據歷史數據，透過移動視窗（Moving Window）技術計算價差（Spread），並進行標準化：
     $Z = \frac{s - \bar{s}}{\sigma_s}$
   - 其中：
     - $s$ 為價差
     - $\bar{s}$ 為歷史移動平均
     - $\sigma_s$ 為歷史標準差

**(3) 參數選擇（Grid Search for Hyperparameters）**
   - 探索最適合的：
     - **視窗大小（Window Size）**
     - **開倉閾值（Open Threshold）**
     - **平倉閾值（Close Threshold）**
   - 透過網格搜索（Grid Search）測試不同參數組合，最終選擇 **開倉閾值 = 1.8 z-score、平倉閾值 = 0.4 z-score、視窗大小 = 900 間隔** 作為最佳參數。

**2. 強化學習交易策略開發**
**(1) RL 環境設計**
   - 建立 RL 環境，讓智能代理學習決策，環境的狀態（State）、行動（Action）、獎勵（Reward）設計如下：
     - **觀察空間（State Space）**：包含**倉位（Position）、價差（Spread）、交易區域（Zone）**等資訊。
     - **行動空間（Action Space）**：
       - **RL1**：僅決定交易時機（開倉、平倉）。
       - **RL2**：同時決定交易時機與投資數量（資本動態分配）。
     - **獎勵函數（Reward Function）**：
       - **交易獲利獎勵**：基於交易獲利計算（Profit Reward）。
       - **行動獎勵**：鼓勵適當的交易決策。
       - **交易成本懲罰**：對於頻繁交易產生負面獎勵，避免過度交易。

**(2) 強化學習演算法選擇**
   - 測試不同 RL 演算法：
     - **決定交易時機（RL1）**：
       - **Proximal Policy Optimization（PPO）**
       - **Advantage Actor-Critic（A2C）**
       - **Deep Q-Network（DQN）**
     - **決定交易時機與投資數量（RL2）**：
       - **PPO**
       - **A2C**
       - **Soft Actor-Critic（SAC）**

   - **演算法優化方式**：
     - 使用 Grid Search 方法調整超參數（Hyperparameter）。
     - 透過歷史數據回測（Backtesting）選出最優演算法。

**3. 交易績效評估**
**(1) 交易績效指標**
   - **累積報酬率（Cumulative Return）**
   - **年化報酬率（CAGR）**
   - **夏普比率（Sharpe Ratio）**：
     $Sharpe Ratio = \frac{R_p - R_f}{\sigma_p}$
     - $R_p$：投資組合報酬率
     - $R_f$：無風險利率（本研究採用 5.5%）
     - $\sigma_p$：投資組合超額報酬的標準差

**(2) 交易行為分析**
   - **交易次數（Total Action Count）**
   - **勝/負交易比率（Win/Loss Action Ratio）**
   - **最大單筆盈利與虧損（Max Win/Loss Action）**
   - **平均交易利潤（Avg Win Action Profit/Loss）**
   - **市場參與時間（Time in Market）**

**(3) 風險評估**
   - **波動性（Annualized Volatility）**
   - **偏態（Skewness）**
   - **峰度（Kurtosis）**

**4. 實驗與比較**
**(1) 交易策略比較**
   - **與傳統方法比較**：
     - **Gatev et al. (2006) 配對交易**（固定閾值規則）
     - **Kim and Kim (2019) RL 配對交易**（閾值自適應）
   - **比較項目**：
     - **RL 方法是否能超越傳統配對交易**？
     - **動態投資數量（RL2）是否比固定資金（RL1）更有效**？
     - **不同 RL 演算法的交易績效比較**？

**(2) 交易成本影響分析**
   - 測試不同交易成本對 RL 交易方法的影響（0.05%、0.02%、0.01%、0%）。
   - 分析交易成本對 RL1 和 RL2 交易策略的影響，檢驗高頻交易環境下 RL 方法的可行性。

**總結**
本研究的核心方法包括：
1. **建立統計套利（配對交易）模型**，確保交易策略有理論基礎。
2. **設計 RL 環境與獎勵機制**，並測試不同 RL 演算法的表現。
3. **採用歷史數據回測並進行績效評估**，比較 RL 方法與傳統交易技術的優劣。
4. **進行交易成本與市場適應性分析**，確保 RL 交易技術在不同交易條件下的適用性。

**研究結果顯示，RL 交易方法（特別是 RL2）能顯著提升交易績效，並在加密貨幣市場等高波動環境下表現優異。**

<a name="S4"></a>
## 4. **研究結果**

**1. RL 交易策略優於傳統方法**
- **傳統配對交易（Gatev et al., 2006）**：  
  - 年化收益率（CAGR）：**195.12%**  
  - 累積收益：**8.33%**
  - **穩定但缺乏適應性**，基於靜態規則進行交易，對市場變動的適應性有限。

- **RL1（決定交易時機）**：
  - **最優 RL1（A2C 演算法）年化收益率達 278.72%**，顯著高於傳統方法。
  - RL 代理能適應市場波動，自動調整進場與離場時機。

- **RL2（決定交易時機與投資數量）**：
  - **最優 RL2（A2C 演算法）年化收益率達 3974.65%**，為所有策略中最高。
  - **累積收益達 31.53%**，相比傳統方法提高超過 **3.78 倍**。
  - 允許 RL 動態調整投資數量，使其能夠根據市場情況最佳化資金分配，進一步提升交易績效。

**2. RL 交易策略的市場參與度更高**
- **交易頻率**：
  - 傳統方法交易次數：**490 次**  
  - RL1（A2C）交易次數：**249 次**（較少但更準確）
  - RL2（A2C）交易次數：**2798 次**（增加市場參與度，提升回報）

- **勝率比較**：
  - **傳統方法**：勝率 **58%（284 勝 / 206 負）**
  - **RL1（A2C）**：勝率 **96%（240 勝 / 9 負）**
  - **RL2（A2C）**：勝率 **93%（917 勝 / 67 負）**

**RL 方法交易更為精準，勝率顯著提升，特別是 RL2 能夠通過動態資金分配優化交易決策。**

**3. 交易成本對 RL 影響分析**
- 測試不同交易成本（0.05%、0.02%、0.01%、0%），結果顯示：
  - **交易成本下降時，RL2 方法表現尤為突出**。
  - **當交易成本降低至 0% 時，RL2 累積收益達 80.92%**，遠超傳統方法（10.54%）。
  - RL 方法的優勢在於高頻交易環境下，若交易成本過高，頻繁交易可能會侵蝕利潤。

**4. 風險管理與回報分析**
- **波動性（年化標準差）**：
  - **傳統方法：6.01%**
  - **RL1（A2C）：6.30%**
  - **RL2（A2C）：27.30%**
  - RL2 交易策略帶來更高的波動性，但因資金動態調整，使風險與回報相匹配。

- **夏普比率（Sharpe Ratio）**：
  - **傳統方法：25.91**
  - **RL1（A2C）：32.74**
  - **RL2（A2C）：94.34**
  - RL2 方法的風險調整後回報明顯更高，顯示在收益增長的同時風險仍可控。

**5. RL2 方法的優勢**
- **高收益：年化回報達 3974.65%（遠超傳統方法 195.12%）**
- **勝率高達 93%（傳統方法僅 58%）**
- **可適應市場變動，優化資金分配，提高交易決策靈活度**
- **當交易成本降低時，盈利能力進一步提升**

**結論**
1. **RL 交易策略優於傳統配對交易方法**，特別是在**高波動市場（如加密貨幣）**中更具優勢。
2. **RL1 交易策略（決定交易時機）能提供穩健回報**，但 RL2 **（決定交易時機與投資數量）表現最佳**。
3. **交易成本影響 RL 交易策略的績效**，若成本較低，RL2 交易策略將大幅超越傳統方法。
4. **RL2 方法提供更靈活的市場適應能力，能在動態市場中最大化交易回報**。

**最終，本研究證明 RL 方法，特別是 RL2，能夠顯著提升配對交易的收益率，使其成為高波動市場中的有效交易策略。**


<a name="S5"></a>
## 5. **研究結論**

該文獻《Reinforcement Learning Pair Trading: A Dynamic Scaling Approach》研究了強化學習（Reinforcement Learning, RL）在加密貨幣市場的配對交易（Pair Trading）中的應用，並提出了一種動態調整的交易方法。以下是其主要結論：

1. **強化學習提升交易績效**：
   - 本研究比較了傳統的配對交易技術與基於強化學習的交易方法，結果顯示 RL 可以顯著提高收益率。在 BTC-GBP 和 BTC-EUR 配對的實驗中，傳統配對交易方法的年化利潤為 **8.33%**，而 RL 配對交易方法的年化利潤範圍則從 **9.94% 到 31.53%**，具體收益率取決於選擇的 RL 演算法。

2. **強化學習方法的優勢**：
   - RL 方法能夠自動適應市場波動，超越傳統基於靜態規則的交易策略。
   - 研究中測試了不同的 RL 演算法（如 A2C、PPO、SAC、DQN），結果顯示 **A2C 演算法最具穩定性和盈利能力**。

3. **動態調整的 RL 配對交易更具競爭力**：
   - 本研究提出兩種 RL 配對交易模式：
     - **RL1**：僅決定交易時機。
     - **RL2**：同時決定交易時機和投資數量（動態調整資金分配）。
   - RL2 配對交易的年化收益率顯著高於 RL1，說明 **允許 RL 自主決策交易數量能進一步提高利潤**。

4. **影響交易策略績效的因素**：
   - **交易成本**：RL 方法在交易成本較低時表現最佳。當交易成本降低到 0% 時，RL2 交易方法的累積收益高達 **80.92%**，遠超傳統方法（10.54%）。
   - **市場波動性**：RL1 和 RL2 都比傳統方法更能適應市場變動，並能在高波動市場中獲取穩定收益。

5. **研究的局限與未來工作**：
   - 研究數據範圍相對有限，可擴展至更長時間框架及更多資產。
   - 目前的配對交易方法僅限於兩資產組合，未來可擴展至多資產組合。
   - 強化學習方法的計算需求較高，未來可研究更高效的訓練方法。

**總結**
該研究證明了 RL 在配對交易中的有效性，尤其是動態調整資金分配的 RL2 方法，顯著提升了交易績效。研究結果表明，利用 RL 技術可以改善決策過程，適應市場變化，提高配對交易的盈利能力，特別是在波動性較大的市場如加密貨幣市場中。
